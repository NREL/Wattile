{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "from wattile.entry_point import init_logging, create_input_dataframe, run_model\n",
    "from wattile.data_reading import read_dataset_from_file\n",
    "from wattile.buildings_processing import correct_predictor_columns, correct_timestamps, rolling_stats, input_data_split\n",
    "from wattile.time_processing import add_processed_time_columns\n",
    "from wattile.entry_point import run_model\n",
    "\n",
    "\n",
    "PROJECT_DIRECTORY = Path().resolve().parent.parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For this example, we will be using the default configs.\n",
    "Check out the docs for an explaination of each config.\n",
    "\"\"\"\n",
    "##################################################################################\n",
    "# choose the configs file to use as an input\n",
    "##################################################################################\n",
    "# # main configs file\n",
    "# with open(PROJECT_DIRECTORY / \"wattile\" / \"configs\" / \"configs.json\", \"r\") as f:\n",
    "#     configs = json.load(f)\n",
    "##################################################################################\n",
    "# code testing configs file\n",
    "with open(PROJECT_DIRECTORY / \"tests\" / \"fixtures\" / \"test_configs.json\", \"r\") as f:\n",
    "    configs = json.load(f)\n",
    "##################################################################################\n",
    "\n",
    "exp_dir = PROJECT_DIRECTORY / \"notebooks\" / \"exp_dir\"\n",
    "if exp_dir.exists():\n",
    "    shutil.rmtree(exp_dir)\n",
    "exp_dir.mkdir()\n",
    "\n",
    "configs[\"exp_dir\"] = str(exp_dir)\n",
    "configs[\"data_dir\"] = str(PROJECT_DIRECTORY / \"data\")\n",
    "\n",
    "configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatype = \"complete example data\" # complete example data, incomplete example data, incomplete small example data\n",
    "incompleteness = True\n",
    "# col_test = ['Synthetic Weather Station Direct Normal Irradiance']\n",
    "col_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if datatype == \"complete example data\":\n",
    "    \"\"\"\n",
    "    Firstly, we will read the raw data from the dataset. \n",
    "    Checkout the docs for an indepth explaination of necessary dataset structure.\n",
    "    \"\"\"\n",
    "    data = read_dataset_from_file(configs)\n",
    "    data\n",
    "    \n",
    "    if incompleteness == True:\n",
    "        \n",
    "        # data_temp = data.loc[\"2021-12-01\":\"2021-12-01\" :,].copy()\n",
    "        data_temp = data.copy()\n",
    "        data_temp\n",
    "\n",
    "        # adding irregular measurement intervals\n",
    "        list_cols = ['Synthetic Weather Station Dew Point Temperature', 'Synthetic Weather Station Diffuse Horizontal Irradiance', 'Synthetic Weather Station Global Horizontal Irradiance']\n",
    "        list_interval_mins = [3, 5, 7]\n",
    "        list_timeshift_mins = [0, 3, 7]\n",
    "        \n",
    "        i=0\n",
    "    \n",
    "        for col, timestep, loffset in zip(list_cols, list_interval_mins, list_timeshift_mins):\n",
    "\n",
    "            print(\"resampling and shifting column = {} with resampling timestep of {} and time-shift of {}\".format(col, timestep, loffset))\n",
    "\n",
    "            minutes = str(timestep) + \"T\"\n",
    "            loffset = str(loffset) + \"min\" \n",
    "            df_temp = data_temp[col].resample(minutes).mean().copy()\n",
    "            df_temp.index = df_temp.index + to_offset(loffset)\n",
    "            data_temp[col] = df_temp\n",
    "\n",
    "        # adding NaNs in random places\n",
    "        fraction = 0.1\n",
    "        list_index_random = data_temp.sample(frac=fraction, replace=False, random_state=1).index.tolist()\n",
    "        list_column_random = pd.DataFrame(data_temp.columns).sample(frac=fraction, replace=False, random_state=2).iloc[:,0].tolist()\n",
    "\n",
    "        i=0\n",
    "        for ind in list_index_random:\n",
    "\n",
    "            for col in list_column_random:\n",
    "\n",
    "                #print(\"replacing value in index = {} and column = {} to blank\".format(ind, col))\n",
    "                data_temp.loc[ data_temp.index==ind , data_temp.columns==col ] = np.NAN\n",
    "                \n",
    "        # adding irregular/random timestamps\n",
    "        def random_dates(start, end, n):\n",
    "\n",
    "            start_u = start.value//10**9\n",
    "            end_u = end.value//10**9\n",
    "\n",
    "            return pd.to_datetime(np.random.randint(start_u, end_u, n), unit='s')\n",
    "        \n",
    "        np.random.seed(seed=1)\n",
    "        start = data_temp.index[0]\n",
    "        end = data_temp.index[-1]\n",
    "        n = data_temp.shape[0]\n",
    "        datetime_random = random_dates(start, end, n)\n",
    "        datetime_random = datetime_random.sort_values()\n",
    "        datetime_random\n",
    "        data_temp.index = datetime_random\n",
    "        \n",
    "        if col_test==[]:\n",
    "            data_test = data_temp.copy()\n",
    "        else:\n",
    "            data_test = data_temp.loc[:, data_temp.columns.isin(col_test)]\n",
    "            \n",
    "elif datatype == \"incomplete small example data1\":\n",
    "\n",
    "    data_test = [\n",
    "        [\n",
    "            \"01:00:00\",\n",
    "            \"01:01:53\",\n",
    "            \"01:03:17\",\n",
    "            \"01:04:02\",\n",
    "            \"01:04:59\",\n",
    "            \"01:05:00\",\n",
    "            \"01:06:22\",\n",
    "            \"01:09:46\",\n",
    "            \"01:10:00\",\n",
    "            \"01:11:22\",\n",
    "            \"01:13:44\",\n",
    "            \"01:14:26\",\n",
    "            \"01:15:00\"\n",
    "        ],\n",
    "        [np.nan, 1.5, 2.2, 0.9, 3.6, np.nan, 3.3, 2.3, np.nan, 1.3, 4.3, 4.1, np.nan],\n",
    "        [1.0, np.nan, np.nan, np.nan, np.nan, 2.0, np.nan, np.nan, 3.0, np.nan, np.nan, np.nan, 4.0]\n",
    "    ]\n",
    "\n",
    "    data_test = pd.DataFrame(data_test).T\n",
    "    data_test.columns = ['ts', 'var1', 'var2']\n",
    "    data_test['var1'] = data_test['var1'].astype(float)\n",
    "    data_test['var2'] = data_test['var2'].astype(float)\n",
    "    data_test['ts'] = pd.to_datetime(data_test.ts)\n",
    "    data_test = data_test.set_index('ts')\n",
    "    \n",
    "elif datatype == \"incomplete small example data2\":\n",
    "    data_test = pd.read_csv(\n",
    "        \"../../tests/fixtures/rolling_stats_input.csv\", \n",
    "        index_col=0,\n",
    "    )\n",
    "    data_test['var1'] = pd.to_numeric(data_test['var1'], errors='coerce')\n",
    "    data_test['var2'] = pd.to_numeric(data_test['var2'], errors='coerce')\n",
    "    data_test['var1'] = data_test['var1'].astype(float)\n",
    "    data_test['var2'] = data_test['var2'].astype(float)\n",
    "    data_test.index = pd.to_datetime(data_test.index, exact=False, utc=True)\n",
    "    data_test = data_test[['var1','var2']]\n",
    "    \n",
    "data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-process data including feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert we have the correct columns and order them\n",
    "data = correct_predictor_columns(configs, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort and trim data specified time period\n",
    "data = correct_timestamps(configs, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time-based features\n",
    "data = add_processed_time_columns(data, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = rolling_stats(data, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = input_data_split(data, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test 2D-3D data conversion for S2S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### original version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def generate_windows(data):\n",
    "    \n",
    "    # an important procedure to convert 2-dimensional data into 3-dimensional for modeling\n",
    "    \n",
    "    x_train = []\n",
    "    y_usage_train = []\n",
    "    x_test = []\n",
    "    y_usage_test = []\n",
    "\n",
    "    # for training data\n",
    "    idxs = np.random.choice(\n",
    "        train_source.shape[0] - (WINDOW_SOURCE_SIZE + WINDOW_TARGET_SIZE),\n",
    "        train_source.shape[0] - (WINDOW_SOURCE_SIZE + WINDOW_TARGET_SIZE),\n",
    "        replace=False,\n",
    "    )\n",
    "\n",
    "    for idx in idxs:\n",
    "        x_train.append(\n",
    "            train_source[idx : idx + WINDOW_SOURCE_SIZE].reshape(\n",
    "                (1, WINDOW_SOURCE_SIZE, train_source.shape[1])\n",
    "            )\n",
    "        )\n",
    "        y_usage_train.append(\n",
    "            train_source[\n",
    "                idx\n",
    "                + WINDOW_SOURCE_SIZE : idx\n",
    "                + WINDOW_SOURCE_SIZE\n",
    "                + WINDOW_TARGET_SIZE,\n",
    "                -1,\n",
    "            ].reshape((1, WINDOW_TARGET_SIZE, 1))\n",
    "        )\n",
    "\n",
    "    x_train = np.concatenate(x_train, axis=0)  # make them arrays and not lists\n",
    "    y_usage_train = np.concatenate(y_usage_train, axis=0)\n",
    "\n",
    "    # for testing data\n",
    "    idxs = np.arange(\n",
    "        0,\n",
    "        len(test_source) - (WINDOW_SOURCE_SIZE + WINDOW_TARGET_SIZE),\n",
    "        WINDOW_TARGET_SIZE,\n",
    "    )\n",
    "\n",
    "    for idx in idxs:\n",
    "        x_test.append(\n",
    "            test_source[idx : idx + WINDOW_SOURCE_SIZE].reshape(\n",
    "                (1, WINDOW_SOURCE_SIZE, test_source.shape[1])\n",
    "            )\n",
    "        )\n",
    "        y_usage_test.append(\n",
    "            test_source[\n",
    "                idx\n",
    "                + WINDOW_SOURCE_SIZE : idx\n",
    "                + WINDOW_SOURCE_SIZE\n",
    "                + WINDOW_TARGET_SIZE,\n",
    "                -1,\n",
    "            ].reshape((1, WINDOW_TARGET_SIZE, 1))\n",
    "        )\n",
    "\n",
    "    x_test = np.concatenate(x_test, axis=0)  # make them arrays and not lists\n",
    "    y_usage_test = np.concatenate(y_usage_test, axis=0)\n",
    "\n",
    "    return x_train, y_usage_train, x_test, y_usage_test\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### updated version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_source_size = 12\n",
    "window_target_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.values\n",
    "val_df = val_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictor = []\n",
    "train_target = []\n",
    "valid_predictor = []\n",
    "valid_target = []\n",
    "\n",
    "# for training data\n",
    "idxs = np.random.choice(\n",
    "    train_df.shape[0] - (window_source_size + window_target_size),\n",
    "    train_df.shape[0] - (window_source_size + window_target_size),\n",
    "    replace=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in idxs:\n",
    "    train_predictor.append(\n",
    "        train_df[idx : idx + window_source_size].reshape(\n",
    "            (1, window_source_size, train_df.shape[1])\n",
    "        )\n",
    "    )\n",
    "    train_target.append(\n",
    "        train_df[\n",
    "            idx\n",
    "            + window_source_size : idx\n",
    "            + window_source_size\n",
    "            + window_target_size,\n",
    "            -1,\n",
    "        ].reshape((1, window_target_size, 1))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictor = np.concatenate(train_predictor, axis=0)  # make them arrays and not lists\n",
    "train_target = np.concatenate(train_target, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for validation data\n",
    "idxs = np.arange(\n",
    "    0,\n",
    "    len(val_df) - (window_source_size + window_target_size),\n",
    "    window_target_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in idxs:\n",
    "    valid_predictor.append(\n",
    "        val_df[idx : idx + window_source_size].reshape(\n",
    "            (1, window_source_size, val_df.shape[1])\n",
    "        )\n",
    "    )\n",
    "    valid_target.append(\n",
    "        val_df[\n",
    "            idx\n",
    "            + window_source_size : idx\n",
    "            + window_source_size\n",
    "            + window_target_size,\n",
    "            -1,\n",
    "        ].reshape((1, window_target_size, 1))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(valid_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(valid_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_predictor = np.concatenate(valid_predictor, axis=0)  # make them arrays and not lists\n",
    "valid_target = np.concatenate(valid_target, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(valid_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(valid_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cleaned version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_full_data_s2s(train_df, val_df, configs):\n",
    "    \n",
    "    # reading configuration settings\n",
    "    window_source_size = 12 # TODO: replace this with configs param\n",
    "    window_target_size = 2 # TODO: replace this with configs param\n",
    "    \n",
    "    train_df = train_df.values\n",
    "    val_df = val_df.values\n",
    "    \n",
    "    # initialize lists\n",
    "    train_predictor = []\n",
    "    train_target = []\n",
    "    valid_predictor = []\n",
    "    valid_target = []\n",
    "\n",
    "    # create rolling window data for both predictor and target and for training data set\n",
    "    idxs = np.random.choice(\n",
    "        train_df.shape[0] - (window_source_size + window_target_size),\n",
    "        train_df.shape[0] - (window_source_size + window_target_size),\n",
    "        replace=False,\n",
    "    )\n",
    "    for idx in idxs:\n",
    "        train_predictor.append(\n",
    "            train_df[idx : idx + window_source_size].reshape(\n",
    "                (1, window_source_size, train_df.shape[1])\n",
    "            )\n",
    "        )\n",
    "        train_target.append(\n",
    "            train_df[\n",
    "                idx\n",
    "                + window_source_size : idx\n",
    "                + window_source_size\n",
    "                + window_target_size,\n",
    "                -1,\n",
    "            ].reshape((1, window_target_size, 1))\n",
    "        )\n",
    "    # convert to numpy array\n",
    "    train_predictor = np.concatenate(train_predictor, axis=0)\n",
    "    train_target = np.concatenate(train_target, axis=0)\n",
    "    \n",
    "    # create rolling window data for both predictor and target and for validation data set\n",
    "    idxs = np.arange(\n",
    "        0,\n",
    "        len(val_df) - (window_source_size + window_target_size),\n",
    "        window_target_size,\n",
    "    )\n",
    "    for idx in idxs:\n",
    "        valid_predictor.append(\n",
    "            val_df[idx : idx + window_source_size].reshape(\n",
    "                (1, window_source_size, val_df.shape[1])\n",
    "            )\n",
    "        )\n",
    "        valid_target.append(\n",
    "            val_df[\n",
    "                idx\n",
    "                + window_source_size : idx\n",
    "                + window_source_size\n",
    "                + window_target_size,\n",
    "                -1,\n",
    "            ].reshape((1, window_target_size, 1))\n",
    "        )\n",
    "    # convert to numpy array\n",
    "    valid_predictor = np.concatenate(valid_predictor, axis=0)  # make them arrays and not lists\n",
    "    valid_target = np.concatenate(valid_target, axis=0)\n",
    "    \n",
    "    return train_predictor, train_target, valid_predictor, valid_target"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "watt",
   "language": "python",
   "name": "watt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
