{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter as sg_filter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn import init\n",
    "from dateutil import parser\n",
    "from pathlib import Path\n",
    "import json \n",
    "import shutil\n",
    "\n",
    "from wattile.data_reading import read_dataset_from_file\n",
    "from wattile.buildings_processing import correct_predictor_columns, correct_timestamps, resample_or_rolling_stats, input_data_split\n",
    "from wattile.time_processing import add_processed_time_columns\n",
    "from wattile.models.liangs_model import main as liangs_model\n",
    "PROJECT_DIRECTORY = Path().resolve().parent.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "# building the S2S model\n",
    "class S2S_Model(nn.Module):\n",
    "    def __init__(self, cell_type, input_size, hidden_size, use_cuda):\n",
    "        super(S2S_Model, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell_type = cell_type\n",
    "\n",
    "        if self.cell_type not in ['rnn', 'gru', 'lstm']:\n",
    "            raise ValueError(self.cell_type, \" is not an appropriate cell type. Please select one of rnn, gru, or lstm.\")\n",
    "        if self.cell_type == 'rnn':\n",
    "            self.Ecell = nn.RNNCell(self.input_size, self.hidden_size)\n",
    "            self.Dcell = nn.RNNCell(1, self.hidden_size)\n",
    "        if self.cell_type == 'gru':\n",
    "            self.Ecell = nn.GRUCell(self.input_size, self.hidden_size)\n",
    "            self.Dcell = nn.GRUCell(1, self.hidden_size)\n",
    "        if self.cell_type == 'lstm':\n",
    "            self.Ecell = nn.LSTMCell(self.input_size, self.hidden_size)\n",
    "            self.Dcell = nn.LSTMCell(1, self.hidden_size)\n",
    "\n",
    "        self.lin_usage = nn.Linear(self.hidden_size, 1)\n",
    "        self.use_cuda = use_cuda\n",
    "        self.init()\n",
    "\n",
    "    # function to intialize weight parameters. Refer to Saxe at al. paper that explains why to use orthogonal init weights\n",
    "    def init(self):\n",
    "        if self.cell_type == 'rnn' or self.cell_type == 'gru':\n",
    "            for p in self.parameters():\n",
    "                if p.dim() > 1:\n",
    "                    init.orthogonal_(p.data, gain=1.0)\n",
    "                if p.dim() == 1:\n",
    "                    init.constant_(p.data, 0.0)\n",
    "        elif self.cell_type == 'lstm':\n",
    "            for p in self.parameters():\n",
    "                if p.dim() > 1:\n",
    "                    init.orthogonal_(p.data, gain=1.0)\n",
    "                if p.dim() == 1:\n",
    "                    init.constant_(p.data, 0.0)\n",
    "                    init.constant_(p.data[self.hidden_size:2*self.hidden_size], 1.0)\n",
    "\n",
    "    def consume(self, x):\n",
    "        # encoder forward function\n",
    "        # for rnn and gru\n",
    "        if self.cell_type == 'rnn' or self.cell_type == 'gru':\n",
    "            h = torch.zeros(x.shape[0], self.hidden_size)\n",
    "            if self.use_cuda:\n",
    "                h = h.cuda()\n",
    "            for T in range(x.shape[1]):\n",
    "                h = self.Ecell(x[:, T, :], h)\n",
    "            pred_usage = self.lin_usage(h)\n",
    "        # for lstm\n",
    "        elif self.cell_type == 'lstm':\n",
    "            h0 = torch.zeros(x.shape[0], self.hidden_size)\n",
    "            c0 = torch.zeros(x.shape[0], self.hidden_size)\n",
    "            if self.use_cuda:\n",
    "                h0 = h0.cuda()\n",
    "                c0 = c0.cuda()\n",
    "            h = (h0, c0)\n",
    "            for T in range(x.shape[1]):\n",
    "                h = self.Ecell(x[:, T, :], h)\n",
    "            pred_usage = self.lin_usage(h[0])\n",
    "        return pred_usage, h\n",
    "\n",
    "    def predict(self, pred_usage, h, target_length):\n",
    "        # decoder forward function\n",
    "        preds = []\n",
    "        # for rnn and gru\n",
    "        if self.cell_type == 'rnn' or self.cell_type == 'gru':\n",
    "            for step in range(target_length):\n",
    "                h = self.Dcell(pred_usage, h)\n",
    "                pred_usage = self.lin_usage(h)\n",
    "                preds.append(pred_usage.unsqueeze(1))\n",
    "            preds = torch.cat(preds, 1)\n",
    "        # for lstm\n",
    "        elif self.cell_type == 'lstm':\n",
    "            for step in range(target_length):\n",
    "                h = self.Dcell(pred_usage, h)\n",
    "                pred_usage = self.lin_usage(h[0])\n",
    "                preds.append(pred_usage.unsqueeze(1))\n",
    "            preds = torch.cat(preds, 1)\n",
    "        return preds\n",
    "\n",
    "#########################################################################################\n",
    "# Bahdanau Attention model\n",
    "# refer to : AuCson github code\n",
    "# building the model\n",
    "class S2S_BA_Model(nn.Module):\n",
    "    def __init__(self, cell_type, input_size, hidden_size, use_cuda):\n",
    "        super(S2S_BA_Model, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_cuda = use_cuda\n",
    "        self.cell_type = cell_type\n",
    "\n",
    "        if self.cell_type not in ['rnn', 'gru', 'lstm']:\n",
    "            raise ValueError(self.cell_type, \" is not an appropriate cell type. Please select one of rnn, gru, or lstm.\")\n",
    "        if self.cell_type == 'rnn':\n",
    "            self.Ecell = nn.RNNCell(self.input_size, self.hidden_size)\n",
    "            self.Dcell = nn.RNNCell(1+self.hidden_size, self.hidden_size)\n",
    "        if self.cell_type == 'gru':\n",
    "            self.Ecell = nn.GRUCell(self.input_size, self.hidden_size)\n",
    "            self.Dcell = nn.GRUCell(1+self.hidden_size, self.hidden_size)\n",
    "        if self.cell_type == 'lstm':\n",
    "            self.Ecell = nn.LSTMCell(self.input_size, self.hidden_size)\n",
    "            self.Dcell = nn.LSTMCell(1+self.hidden_size, self.hidden_size)\n",
    "\n",
    "        self.Wattn_energies = nn.Linear(self.hidden_size*2, self.hidden_size)\n",
    "        self.Wusage = nn.Linear(self.hidden_size, 1)\n",
    "        self.Wout = nn.Linear(1+self.hidden_size*2, self.hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(self.hidden_size))\n",
    "        stdv = 1./math.sqrt(self.v.size(0))\n",
    "        self.v.data.normal_(mean=0, std=stdv)\n",
    "        self.init()\n",
    "\n",
    "# function to intialize weight parameters\n",
    "    def init(self):\n",
    "        if self.cell_type == 'rnn' or self.cell_type == 'gru':\n",
    "            for p in self.parameters():\n",
    "                if p.dim() > 1:\n",
    "                    init.orthogonal_(p.data, gain=1.0)\n",
    "                if p.dim() == 1:\n",
    "                    init.constant_(p.data, 0.0)\n",
    "        elif self.cell_type == 'lstm':\n",
    "            for p in self.parameters():\n",
    "                if p.dim() > 1:\n",
    "                    init.orthogonal_(p.data, gain=1.0)\n",
    "                if p.dim() == 1:\n",
    "                    init.constant_(p.data, 0.0)\n",
    "                    init.constant_(p.data[self.hidden_size:2*self.hidden_size], 1.0)\n",
    "\n",
    "    def consume(self, x):\n",
    "        # for rnn and gru\n",
    "        if self.cell_type == 'rnn' or self.cell_type == 'gru':\n",
    "            # encoder forward function\n",
    "            h = torch.zeros(x.shape[0], self.hidden_size)\n",
    "            encoder_outputs = torch.zeros(x.shape[1], x.shape[0], self.hidden_size)\n",
    "            if self.use_cuda:\n",
    "                h = h.cuda()\n",
    "                encoder_outputs = encoder_outputs.cuda()\n",
    "            # encoder part\n",
    "            for T in range(x.shape[1]):\n",
    "                h = self.Ecell(x[:, T, :], h)\n",
    "                encoder_outputs[T] = h\n",
    "            pred_usage = self.Wusage(h)\n",
    "        # for lstm\n",
    "        elif self.cell_type == 'lstm':\n",
    "            h0 = torch.zeros(x.shape[0], self.hidden_size)\n",
    "            c0 = torch.zeros(x.shape[0], self.hidden_size)\n",
    "            encoder_outputs = torch.zeros(x.shape[1], x.shape[0], self.hidden_size)\n",
    "            if self.use_cuda:\n",
    "                h0 = h0.cuda()\n",
    "                c0 = c0.cuda()\n",
    "                encoder_outputs = encoder_outputs.cuda()\n",
    "            h = (h0, c0)\n",
    "            for T in range(x.shape[1]):\n",
    "                h = self.Ecell(x[:, T, :], h)\n",
    "                encoder_outputs[T] = h[0]\n",
    "            pred_usage = self.Wusage(h[0])\n",
    "        return pred_usage, h, encoder_outputs\n",
    "\n",
    "    def predict(self, pred_usage, h, encoder_outputs, target_length):\n",
    "        # decoder with attention function\n",
    "        preds = []\n",
    "        # for rnn and gru\n",
    "        if self.cell_type == 'rnn' or self.cell_type == 'gru':\n",
    "            for step in range(target_length):\n",
    "                h_copies = h.expand(encoder_outputs.shape[0], -1, -1)\n",
    "                energies = torch.tanh(self.Wattn_energies(torch.cat((h_copies, encoder_outputs), 2)))\n",
    "                score = torch.sum(self.v * energies, dim=2)\n",
    "                attn_weights = score.t()\n",
    "                attn_weights = torch.softmax(attn_weights, dim=1).unsqueeze(1)\n",
    "                context = attn_weights.bmm(encoder_outputs.transpose(0,1)).squeeze(1)\n",
    "                gru_input = torch.cat((pred_usage, context), 1)\n",
    "                h = self.Dcell(gru_input, h)\n",
    "                output = self.Wout(torch.cat((pred_usage, h, context), 1))\n",
    "                pred_usage = self.Wusage(output)\n",
    "                preds.append(pred_usage.unsqueeze(1))\n",
    "            preds = torch.cat(preds, 1)\n",
    "        # for lstm\n",
    "        elif self.cell_type == 'lstm':\n",
    "            for step in range(target_length):\n",
    "                h_copies = h[0].expand(encoder_outputs.shape[0], -1, -1)\n",
    "                energies = torch.tanh(self.Wattn_energies(torch.cat((h_copies, encoder_outputs), 2)))\n",
    "                score = torch.sum(self.v * energies, dim=2)\n",
    "                attn_weights = score.t()\n",
    "                attn_weights = torch.softmax(attn_weights, dim=1).unsqueeze(1)\n",
    "                context = attn_weights.bmm(encoder_outputs.transpose(0,1)).squeeze(1)\n",
    "                gru_input = torch.cat((pred_usage, context), 1)\n",
    "                h = self.Dcell(gru_input, h)\n",
    "                output = self.Wout(torch.cat((pred_usage, h[0], context), 1))\n",
    "                pred_usage = self.Wusage(output)\n",
    "                preds.append(pred_usage.unsqueeze(1))\n",
    "            preds = torch.cat(preds, 1)\n",
    "        return preds\n",
    "\n",
    "#############################################################################################3\n",
    "# Luong Attention module\n",
    "class Attn(torch.nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        \n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \" is not an appropriate attention method, please select one of dot, general, or concat.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        if self.method == 'concat':\n",
    "            self.attn = nn.Linear(2*self.hidden_size, self.hidden_size)\n",
    "            self.v = nn.Parameter(torch.rand(self.hidden_size))\n",
    "            stdv = 1./math.sqrt(self.v.size(0))\n",
    "            self.v.data.normal_(mean=0, std=stdv)\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        attn_energies = torch.sum(hidden*encoder_output, dim=2)\n",
    "        return attn_energies\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        attn_energies = torch.sum(hidden*energy, dim=2)\n",
    "        return attn_energies\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden.expand(encoder_output.shape[0], -1, -1),\n",
    "                            encoder_output), 2)))\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    # calculate the attention weights (energies) based on the given method\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        if self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        attn_energies = attn_energies.t()\n",
    "        attn_weights = torch.softmax(attn_energies, dim=1).unsqueeze(1)\n",
    "        return attn_weights\n",
    "\n",
    "#########################################################################################\n",
    "#  building the S2S LA model\n",
    "class S2S_LA_Model(nn.Module):\n",
    "    def __init__(self, cell_type, attn_method, input_size, hidden_size, use_cuda):\n",
    "        super(S2S_LA_Model, self).__init__()\n",
    "        self.cell_type = cell_type\n",
    "        self.attn_method = attn_method\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        if self.cell_type == 'rnn':\n",
    "            self.Ecell = nn.RNNCell(self.input_size, self.hidden_size)\n",
    "            self.Dcell = nn.RNNCell(1, self.hidden_size)\n",
    "        if self.cell_type == 'gru':\n",
    "            self.Ecell = nn.GRUCell(self.input_size, self.hidden_size)\n",
    "            self.Dcell = nn.GRUCell(1, self.hidden_size)\n",
    "        if self.cell_type == 'lstm':\n",
    "            self.Ecell = nn.LSTMCell(self.input_size, self.hidden_size)\n",
    "            self.Dcell = nn.LSTMCell(1, self.hidden_size)\n",
    "\n",
    "        self.lin_usage = nn.Linear(hidden_size, 1)\n",
    "        self.lin_concat = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.attn = Attn(self.attn_method, self.hidden_size)\n",
    "        self.use_cuda = use_cuda\n",
    "        self.init()\n",
    "\n",
    "    # function to intialize weight parameters\n",
    "    def init(self):\n",
    "        if self.cell_type == 'rnn' or self.cell_type == 'gru':\n",
    "            for p in self.parameters():\n",
    "                if p.dim() > 1:\n",
    "                    init.orthogonal_(p.data, gain=1.0)\n",
    "                if p.dim() == 1:\n",
    "                    init.constant_(p.data, 0.0)\n",
    "        elif self.cell_type == 'lstm':\n",
    "            for p in self.parameters():\n",
    "                if p.dim() > 1:\n",
    "                    init.orthogonal_(p.data, gain=1.0)\n",
    "                if p.dim() == 1:\n",
    "                    init.constant_(p.data, 0.0)\n",
    "                    init.constant_(p.data[self.hidden_size:2*self.hidden_size], 1.0)\n",
    "\n",
    "    def consume(self, x):\n",
    "        if self.cell_type == 'rnn' or self.cell_type == 'gru':\n",
    "            # encoder forward function\n",
    "            h = torch.zeros(x.shape[0], self.hidden_size)\n",
    "            encoder_outputs = torch.zeros(x.shape[1], x.shape[0], self.hidden_size)\n",
    "            if self.use_cuda:\n",
    "                h = h.cuda()\n",
    "                encoder_outputs = encoder_outputs.cuda()\n",
    "            # encoder part\n",
    "            for T in range(x.shape[1]):\n",
    "                h = self.Ecell(x[:, T, :], h)\n",
    "                encoder_outputs[T] = h\n",
    "            pred_usage = self.lin_usage(h)\n",
    "        elif self.cell_type == 'lstm':\n",
    "            h0 = torch.zeros(x.shape[0], self.hidden_size)\n",
    "            c0 = torch.zeros(x.shape[0], self.hidden_size)\n",
    "            encoder_outputs = torch.zeros(x.shape[1], x.shape[0], self.hidden_size)\n",
    "            if self.use_cuda:\n",
    "                h0 = h0.cuda()\n",
    "                c0 = c0.cuda()\n",
    "                encoder_outputs = encoder_outputs.cuda()\n",
    "            h = (h0, c0)\n",
    "            for T in range(x.shape[1]):\n",
    "                h = self.Ecell(x[:, T, :], h)\n",
    "                encoder_outputs[T] = h[0]\n",
    "            pred_usage = self.lin_usage(h[0])\n",
    "        return pred_usage, h, encoder_outputs\n",
    "\n",
    "    def predict(self, pred_usage, h, encoder_outputs, target_length):\n",
    "        # decoder with attention function\n",
    "        preds = []\n",
    "        if self.cell_type == 'rnn' or self.cell_type == 'gru':\n",
    "            for step in range(target_length):\n",
    "                h = self.Dcell(pred_usage, h)\n",
    "                attn_weights = self.attn(h, encoder_outputs)\n",
    "                context = attn_weights.bmm(encoder_outputs.transpose(0,1))\n",
    "                context = context.squeeze(1)\n",
    "                concat_input = torch.cat((h, context), 1)\n",
    "                concat_output = torch.tanh(self.lin_concat(concat_input))\n",
    "                pred_usage = self.lin_usage(concat_output)\n",
    "                preds.append(pred_usage.unsqueeze(1))\n",
    "            preds = torch.cat(preds, 1)\n",
    "        elif self.cell_type == 'lstm':\n",
    "            for step in range(target_length):\n",
    "                h = self.Dcell(pred_usage, h)\n",
    "                attn_weights = self.attn(h[0], encoder_outputs)\n",
    "                context = attn_weights.bmm(encoder_outputs.transpose(0,1))\n",
    "                context = context.squeeze(1)\n",
    "                concat_input = torch.cat((h[0], context), 1)\n",
    "                concat_output = torch.tanh(self.lin_concat(concat_input))\n",
    "                pred_usage = self.lin_usage(concat_output)\n",
    "                preds.append(pred_usage.unsqueeze(1))\n",
    "            preds = torch.cat(preds, 1)\n",
    "        return preds\n",
    "    \n",
    "# loss function, qs here is an integer, not a list of integer\n",
    "def quantile_loss(output, target, qs, window_target_size):\n",
    "    \"\"\"\n",
    "    Computes loss for quantile methods.\n",
    "    :param output: (Tensor)\n",
    "    :param target: (Tensor)\n",
    "    :param qs: (int)\n",
    "    :param window_target_size: (int)\n",
    "    :return: (Tensor) Loss for this study (single number)\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    resid = target - output\n",
    "    tau = torch.tensor([qs], device=device).repeat_interleave(window_target_size)\n",
    "\n",
    "    alpha = 0.001\n",
    "    log_term = torch.zeros_like(resid, device=device)\n",
    "    log_term[resid < 0] = torch.log(1 + torch.exp(resid[resid < 0] / alpha)) - (\n",
    "        resid[resid < 0] / alpha\n",
    "    )\n",
    "    log_term[resid >= 0] = torch.log(1 + torch.exp(-resid[resid >= 0] / alpha))\n",
    "    loss = resid * tau + alpha * log_term\n",
    "    loss = torch.mean(torch.mean(loss, 0))\n",
    "\n",
    "    return loss\n",
    "\n",
    "# A key function to convert from 2D data to 3D data. \n",
    "def generate_windows(train_source, test_source, WINDOW_SOURCE_SIZE, WINDOW_TARGET_SIZE):\n",
    "    x_train = []\n",
    "    y_usage_train = []\n",
    "    x_test = []\n",
    "    y_usage_test = []\n",
    "\n",
    "    # for training data\n",
    "    # idxs = np.random.choice(train_source.shape[0]-(WINDOW_SOURCE_SIZE+WINDOW_TARGET_SIZE), train_source.shape[0]-(WINDOW_SOURCE_SIZE+WINDOW_TARGET_SIZE), replace=False)\n",
    "    idxs = np.arange(0, len(train_source)-(WINDOW_SOURCE_SIZE+WINDOW_TARGET_SIZE))\n",
    "\n",
    "    for idx in idxs:\n",
    "        x_train.append(train_source[idx:idx+WINDOW_SOURCE_SIZE].reshape((1, WINDOW_SOURCE_SIZE, train_source.shape[1])) )\n",
    "        y_usage_train.append(train_source[idx+WINDOW_SOURCE_SIZE:idx+WINDOW_SOURCE_SIZE+WINDOW_TARGET_SIZE, -1].reshape((1, WINDOW_TARGET_SIZE, 1)) )\n",
    "\n",
    "    x_train = np.concatenate(x_train, axis=0) # make them arrays and not lists\n",
    "    y_usage_train = np.concatenate(y_usage_train, axis=0)\n",
    "\n",
    "    # for testing data\n",
    "    idxs = np.arange(0, len(test_source)-(WINDOW_SOURCE_SIZE+WINDOW_TARGET_SIZE), WINDOW_TARGET_SIZE)\n",
    "\n",
    "    for idx in idxs:\n",
    "        x_test.append(test_source[idx:idx+WINDOW_SOURCE_SIZE].reshape((1, WINDOW_SOURCE_SIZE, test_source.shape[1])) )\n",
    "        y_usage_test.append(test_source[idx+WINDOW_SOURCE_SIZE:idx+WINDOW_SOURCE_SIZE+WINDOW_TARGET_SIZE, -1].reshape((1, WINDOW_TARGET_SIZE, 1)) )\n",
    "\n",
    "    x_test = np.concatenate(x_test, axis=0) # make them arrays and not lists\n",
    "    y_usage_test = np.concatenate(y_usage_test, axis=0)\n",
    "\n",
    "    return x_train, y_usage_train, x_test, y_usage_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reading configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For this example, we will be using the default configs.\n",
    "Check out the docs for an explaination of each config.\n",
    "\"\"\"\n",
    "##################################################################################\n",
    "# choose the configs file to use as an input\n",
    "##################################################################################\n",
    "# # main configs file\n",
    "# with open(PROJECT_DIRECTORY / \"wattile\" / \"configs\" / \"configs.json\", \"r\") as f:\n",
    "#     configs = json.load(f)\n",
    "##################################################################################\n",
    "# code testing configs file\n",
    "with open(PROJECT_DIRECTORY / \"tests\" / \"fixtures\" / \"test_configs.json\", \"r\") as f:\n",
    "    configs = json.load(f)\n",
    "##################################################################################\n",
    "\n",
    "exp_dir = PROJECT_DIRECTORY / \"notebooks\" / \"exp_dir\"\n",
    "if exp_dir.exists():\n",
    "    shutil.rmtree(exp_dir)\n",
    "exp_dir.mkdir()\n",
    "\n",
    "configs[\"exp_dir\"] = str(exp_dir)\n",
    "configs[\"num_epochs\"] = 10\n",
    "configs[\"data_dir\"] = str(PROJECT_DIRECTORY / \"tests\" / \"data\")\n",
    "\n",
    "configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### override config if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs[\"feat_stats\"][\"window_width\"] = '15min'\n",
    "# configs[\"feat_stats\"][\"window_increment\"] = '15min'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = configs['building']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_dataset_from_file(configs)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert we have the correct columns and order them\n",
    "data = correct_predictor_columns(configs, data)\n",
    "\n",
    "# sort and trim data specified time period\n",
    "data = correct_timestamps(configs, data)\n",
    "\n",
    "# Add time-based features\n",
    "data = add_processed_time_columns(data, configs)\n",
    "\n",
    "# Add statistics features\n",
    "data = resample_or_rolling_stats(data, configs)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wTS = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =======================================\n",
    "# testing\n",
    "# ======================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading configuration settings\n",
    "configs[\"window_source_size\"] = 12  # TODO: replace this with configs param later\n",
    "configs[\"window_target_size\"] = 3  # TODO: replace this with configs param later\n",
    "configs[\"normalization\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " TODOs:\n",
    " \n",
    " - `roll_full_data_s2s` output to `data`\n",
    " - use above `data` as input to `input_data_split`\n",
    " - connect `val_df_target` properly to results plot\n",
    " - `usage_actual = data[configs[\"target_var\"]]`\n",
    " - test data converted to 3D with every interval instead of interval with window_target_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_wTS.astype(np.float32).copy()\n",
    "\n",
    "####################################################################################\n",
    "# roll_full_data_s2s\n",
    "####################################################################################\n",
    "normalization = configs[\"normalization\"]\n",
    "window_source_size = configs[\"window_source_size\"]\n",
    "window_target_size = configs[\"window_target_size\"]\n",
    "\n",
    "# normalization\n",
    "if normalization:\n",
    "    print(\"Transforming data to 0 mean and unit var\")\n",
    "    MU = dataset.mean(0)  # 0 means take the mean of the column\n",
    "    dataset = dataset - MU\n",
    "    STD = dataset.std(0)  # same with std here\n",
    "    dataset = dataset / STD\n",
    "\n",
    "# initialize lists\n",
    "data_predictor = []\n",
    "data_target = []\n",
    "\n",
    "data = dataset.values\n",
    "\n",
    "# create rolling window data for both predictor and target and for training data set\n",
    "idxs = np.arange(\n",
    "    0,\n",
    "    data.shape[0] - (window_source_size + window_target_size)\n",
    ")\n",
    "for idx in idxs:\n",
    "    data_predictor.append(\n",
    "        data[idx : idx + window_source_size].reshape(\n",
    "            (1, window_source_size, data.shape[1])\n",
    "        )\n",
    "    )\n",
    "    data_target.append(\n",
    "        data[\n",
    "            idx\n",
    "            + window_source_size : idx\n",
    "            + window_source_size\n",
    "            + window_target_size,\n",
    "            -1,\n",
    "        ].reshape((1, window_target_size, 1))\n",
    "    )\n",
    "\n",
    "# convert to numpy array\n",
    "data_predictor = np.concatenate(data_predictor, axis=0)\n",
    "data_target = np.concatenate(data_target, axis=0) \n",
    "\n",
    "# combining 3D predictor and target data into dictionary\n",
    "data = {}\n",
    "data['predictor'] = data_predictor\n",
    "data['target'] = data_target\n",
    "####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_predictor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_wTS.astype(np.float32).copy()\n",
    "\n",
    "# normalization\n",
    "if normalization:\n",
    "    print(\"Transforming data to 0 mean and unit var\")\n",
    "    MU = dataset.mean(0)  # 0 means take the mean of the column\n",
    "    dataset = dataset - MU\n",
    "    STD = dataset.std(0)  # same with std here\n",
    "    dataset = dataset / STD\n",
    "\n",
    "# initialize lists\n",
    "data_predictor = []\n",
    "data_target = []\n",
    "\n",
    "# create rolling window data for both predictor and target and for training data set\n",
    "\"\"\"\n",
    "idxs = np.arange(\n",
    "    0,\n",
    "    data.shape[0] - (window_source_size + window_target_size)\n",
    ")\n",
    "for idx in idxs:\n",
    "    data_predictor.append(\n",
    "        data[idx : idx + window_source_size].reshape(\n",
    "            (1, window_source_size, data.shape[1])\n",
    "        )\n",
    "    )\n",
    "    data_target.append(\n",
    "        data[\n",
    "            idx\n",
    "            + window_source_size : idx\n",
    "            + window_source_size\n",
    "            + window_target_size,\n",
    "            -1,\n",
    "        ].reshape((1, window_target_size, 1))\n",
    "    )\n",
    "\"\"\"\n",
    "for window in dataset.rolling(window=\"180min\", min_periods=0):\n",
    "    data_predictor.append(\n",
    "        window.values.reshape((1, window_source_size, dataset.shape[1])).shape\n",
    "    )\n",
    "    \n",
    "data_set_shifted = dataset.loc[dataset.index>=dataset.shift(freq=\"180min\").index[0], :]\n",
    "for window in data_set_shifted.rolling(window=\"45min\", min_periods=0):\n",
    "    data_target.append(\n",
    "        window.values.reshape((1, window_target_size, dataset.shape[1])).shape\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_shifted = dataset.loc[dataset.index>=dataset.shift(freq=\"180min\").index[0], :]\n",
    "for window in data_set_shifted.rolling(window=\"45min\", min_periods=0):\n",
    "    print(window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "watt",
   "language": "python",
   "name": "watt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
